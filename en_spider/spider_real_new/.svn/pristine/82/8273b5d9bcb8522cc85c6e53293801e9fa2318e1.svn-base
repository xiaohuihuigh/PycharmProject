# -*- coding:utf-8 -*-
# 用途： 新的400w页面爬虫
# 创建日期: 18-9-11 下午7:20

import gevent.monkey
gevent.monkey.patch_all()  # 对python标准库进行修改

import time
import logging
import random
logger = logging.getLogger("spider")
import os
import gevent
from gevent.pool import Pool
from settings import CON, CONCURRENT_ITEMS,  CONCURRENT_PER_REQUESTS, HTML_DIR
from main_request.get_request import Request
from page_parse import content_parse, get_failed
from pipelines import page_pipelines_new
from items import PageNewItem
from xpath_pool import XPATH_DIC
from spider_conf import MOD


class PageSpider(object):
    """
        页面爬虫,用来采集已经有url,但没有采集页面具体信息.
        item 字段说明 news_count 0代表正常采集并解析 1代表解析为空 2代表解析内容过短 -1代表采集失败 -2 代表解析表达式错误
    """

    def __init__(self):
        """ 初始化 """
        self._pool = Pool(CONCURRENT_ITEMS)
        self._site = []
        self._data = []
        self._exist_id = set([])
        self._valid_site_ids = set([])
        self._site_index = 0
        self._data_index = 0
        self._item_count = 0
        self._get_count = 1  # 采集次数
        self._invalid_count = 0
        self._request_count = 0
        self._exist_count = 0
        self._invalid_site = set([])
        self._count = 0
        self._request = Request()

    def _get_page(self, item):
        """ 采集页面并处理"""
        # print "aaaaa"
        if item.get('run_status') > 0:  # 仅采集状态大于0的网站
            article_id = item.get("article_id")
            title = item.get("news_title")
            site_id = item.get("site_id")
            logger.debug("start running article {} site {}".format(article_id, site_id))
            request = self._request.get_class(sid=site_id, mode=True)
            while request.COUNTS > CONCURRENT_PER_REQUESTS:  # 限制发送请求总数量
                gevent.sleep(0)
            # resp = None
            with gevent.Timeout(30, False) as timeout:  # 设置超时时间
                resp = request.run(url=item.get('news_url'), num=3)
            if resp:
                self.save_html_text(html_text=resp.text, title=title, article_id=article_id, site_id=site_id)
                status_code = resp.status_code
                if status_code < 300:
                    pass
                else:
                    logger.info("get html status error {}, {}".format(status_code, article_id))
                content_parse(item.get('site_id'), resp, item)  # 解析内容
            else:
                print "get html failed {}".format(article_id)
                status_code = 404
                item.set('news_count', get_failed)
                # 这里需要标记一个失败动作
            item.set('status', status_code)
            self._data.append(item)

    def save_html_text(self, html_text, title, article_id, site_id):
        file_dir = HTML_DIR
        if file_dir is None:
            print "warning: html_dir is None"
            return
        file_name = u"{}_{}_{}".format(article_id, site_id, "urlnew") + ".html"
        file_path = os.path.join(file_dir, file_name)
        with open(file_path, 'w') as wf:
            wf.write(html_text.encode("utf8"))

    def set_exist_ids(self):
        """获取已经访问过的id,用于去重"""
        cur = CON.cursor()
        cur.execute("select `news_id` from `url_new_content` order by `news_id`")
        for article_id, in cur.fetchall():
            self._exist_id.add(article_id)
        # if len(self._exist_id) != 0:
        #     max_id = max(self._exist_id)
        #     self._site_index = max(max_id, 0)  # 这里可以设置初始值

    def set_valid_site_ids(self):
        """获取有效网址"""
        for site_id, xpath_text in XPATH_DIC.items():
            if xpath_text:
                self._valid_site_ids.add(site_id)

    def set_ignore_sites(self):
        """添加临时过滤网站"""
        ignore_sites = {230, 296, 344, 365}
        self._valid_site_ids = self._valid_site_ids - ignore_sites

    def get_items(self, mod=MOD, task_num=10000, run_status=1):
        """领取采集任务,最好一次领取很多个任务
        ::mod 取模,用于任务分组
        ::task_num 一次获取数量
        ::run_status 运行标志,先设置为1
        """
        items = []
        # print self._site_index

        cur = CON.cursor()
        # cur.execute("SELECT `news_id`, `news_title`, `url`, `news_time`, `site`, `status` FROM `url_new1` "
        #             "WHERE `news_id` > %s ORDER BY `news_id` limit %s",
        #             (self._site_index, task_num))
        site_id = 448
        cur.execute("SELECT `news_id`, `news_title`, `url`, `news_time`, `site`, `status` FROM `url_new1` "
                    "WHERE `news_id` > %s and site = %s ORDER BY `news_id` limit %s",
                    (self._site_index, site_id, task_num))
        res_rows = cur.fetchall()
        if not res_rows:
            print "invalid", self._invalid_count
            print "exist", self._exist_count
            print "invalid site", self._invalid_site
            exit(0)
        for news_id, news_title, url, news_time, site, status in res_rows:
            # 筛选
            self._site_index = news_id
            if news_id % 10 != mod:
                continue
            if site not in self._valid_site_ids:
                # self._invalid_count += 1
                # self._invalid_site.add(site)
                continue
            if news_id in self._exist_id:
                # self._exist_count += 1
                continue
            self._count += 1
            content = ""
            used = self._get_count  # 设置采集次数标记，用于去重
            source = ""
            news_count = 0
            item_dic = {'article_id': news_id, 'site_id': site, 'news_url': url, 'source': source,
                        'news_title': news_title, 'news_count': news_count, 'news_time': news_time,
                        'used': used, 'content': content, 'status': status, 'run_status': run_status}
            item = PageNewItem(**item_dic)
            items.append(item)
        cur.close()
        # 这里最好做一个随机的打乱处理
        # random.shuffle(items)
        return items

    def run(self):
        """ 新闻爬虫主流程入口 """
        self.set_exist_ids()
        self.set_valid_site_ids()
        self.set_ignore_sites()

        timestamp = time.time()
        stop_time = timestamp + 86400 * 10
        self._step = 10 if timestamp % 86400 < 28800 else 60
        self._pipetime = timestamp + 30
        while True and timestamp < stop_time:  # 调试状态,永远运行
            timestamp = time.time()
            self._second = int(timestamp+28800) % 86400
            items = self.get_items()
            for item in items:
                self._pool.add(gevent.spawn(self._get_page, item))  # 这里进行了内容页的抓取
            logger.debug('pool add in loop')
            gevent.sleep(0)  # 交出控制权,其实这句不需要
            # print "{}: waitting task finished {}".format(time.time(), self._site_index)
            self._pool.join()  # 等待任务结束
            page_pipelines_new(self._data)  # 存储采集结果   这里要特别注意 self._data的溢出问题
            del self._data[:]


if __name__ == '__main__':
    print "spider starting..."
    spider = PageSpider()
    spider.run()
    print "spider stop."
